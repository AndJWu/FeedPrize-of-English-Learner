{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/andjmade/fork-of-fb3-english-learning?scriptVersionId=148625376\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import os,gc,re,ast,sys,copy,json,time,datetime,math,string,pickle,random,joblib,itertools\n\nfrom distutils.util import strtobool\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold,train_test_split\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.checkpoint import checkpoint\n\nimport transformers,tokenizers\nprint(f'transformers.__version__: {transformers.__version__}')\nprint(f'tokenizers.__version__: {tokenizers.__version__}')\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\nfrom transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\nos.environ['TOKENIZERS_PARALLELISM']='true'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    model_name = \"microsoft/deberta-v3-base\"\n    model_path = \"../input/microsoftdebertav3large/deberta-v3-base\"\n    \n    \n    batch_size ,n_targets,num_workers = 8,6,4\n    target_cols = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n    epochs,print_freq = 5,20 # 训练时每隔20step打印一次    \n    save_all_models=False # 是否每个epoch都保存数据\n    gradient_checkpointing = True\n    \n    loss_func = 'SmoothL1' # 'SmoothL1', 'RMSE'\n    pooling = 'attention' # mean, max, min, attention, weightedlayer\n    gradient_checkpointing = True\n    gradient_accumulation_steps = 1 # 是否使用梯度累积更新\n    max_grad_norm = 1000 #梯度裁剪\n    apex = True # 是否进行自动混合精度训练 \n    \n    # 启用llrd\n    layerwise_lr,layerwise_lr_decay = 5e-5,0.9\n    layerwise_weight_decay = 0.01\n    layerwise_adam_epsilon = 1e-6\n    layerwise_use_bertadam = False\n    \n    scheduler = 'cosine'\n    num_cycles ,num_warmup_steps= 0.5,0\n    encoder_lr,decoder_lr,min_lr  = 2e-5,2e-5 ,1e-6\n    max_len = 512\n    weight_decay = 0.01\n    \n    fgm = True # 是否使用fgm对抗网络攻击\n    wandb=False\n    adv_lr,adv_eps,eps,betas = 1,0.2,1e-6,(0.9, 0.999)\n    unscale =True\n    \n    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    seed=42\n    n_fold=4\n    trn_fold=list(range(n_fold))\n    debug=False # debug表示只使用少量样本跑代码，且n_fold=2，epoch=2\n    \n    OUTPUT_DIR = f\"./{model_name.replace('/', '-')}/\"\n    train_file = '../input/feedback-prize-english-language-learning/train.csv'\n    test_file = '../input/feedback-prize-english-language-learning/test.csv'\n    submission_file = '../input/feedback-prize-english-language-learning/sample_submission.csv'\n    \nif not os.path.exists(CFG.OUTPUT_DIR):\n    os.makedirs(CFG.OUTPUT_DIR)\n    \nCFG.OUTPUT_DIR","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 设置随机种子 固定结果\ndef set_seeds(seed):\n    random.seed(seed)\n    np.random.seed(seed)  # 保证后续使用random函数时，产生固定的随机数\n    torch.manual_seed(seed)  # 固定随机种子（CPU）\n    if torch.cuda.is_available():  # 固定随机种子（GPU)\n        torch.cuda.manual_seed(seed)  # 为当前GPU设置\n        torch.cuda.manual_seed_all(seed)  # 为所有GPU设置\n    \n    torch.backends.cudnn.deterministic = True  # 固定网络结构\n    \nset_seeds(CFG.seed)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if CFG.wandb:    \n    import wandb\n    try:\n        from kaggle_secrets import UserSecretsClient\n        user_secrets = UserSecretsClient()\n        api_key = user_secrets.get_secret(\"WANDB\")\n        wandb.login(key=api_key)\n    except:\n        wandb.login(anonymous='must')\n        print('To use your W&B account,\\nGo to Add-ons -> Secrets and provide your W&B access token. Use the Label name as WANDB. \\nGet your W&B access token from here: https://wandb.ai/authorize')\n\n    def class2dict(f):\n        return dict((name, getattr(f, name)) for name in dir(f) if not name.startswith('__'))\n\n    run = wandb.init(project='FB3-Public', \n                     name=CFG.model,\n                     config=class2dict(CFG),\n                     group=CFG.model,\n                     job_type=\"train\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\ndef preprocess(df,tokenizer,types=True):\n    if types:\n        labels = np.array(df[[\"cohesion\", \"syntax\", \"vocabulary\", \"phraseology\", \"grammar\", \"conventions\"]])\n    else:\n        labels=df[\"labels\"]\n    text=list(df['full_text'].iloc[:])\n    encoding=tokenizer(text,truncation=True,padding=\"max_length\",\n                        max_length=CFG.max_len,return_tensors=\"np\")#训练集中划分的训练集\n    return encoding,labels\n    \n\ndf=pd.read_csv(CFG.train_file)\ntest_df = pd.read_csv(CFG.test_file)\ntest_df['labels']=None\ntest_df['labels']=test_df['labels'].apply(lambda x:[0,0,0,0,0,0])\n\ntokenizer = AutoTokenizer.from_pretrained(CFG.model_path)\ntest_encoding,test_label=preprocess(test_df,tokenizer,False)\ntest_encoding","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#加载到datalodar并预处理\n#数据集读取\nfrom torch.utils.data import Dataset, DataLoader,TensorDataset\nclass MyDataset(Dataset):\n    def __init__(self,encoding,label):\n        self.inputs=encoding\n        self.label=label\n        \n\n    # 读取单个样本\n    def __getitem__(self,idx):\n        item={key:torch.tensor(val[idx],dtype = torch.long) for key,val in self.inputs.items()}\n        label=torch.tensor(self.label[idx],dtype=torch.float)\n        return item,label\n\n    def __len__(self):\n        return len(self.label)\n\ndef collate(inputs): # 貌似是每个批次选这个批次的最大长度，去掉也没事吧\n    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n    for k, v in inputs.items():\n        inputs[k] = inputs[k][:,:mask_len]\n    return inputs\n\ntest_dataset=MyDataset(test_encoding,test_label)\ntest_loader=DataLoader(test_dataset,batch_size=CFG.batch_size,\n                       num_workers=CFG.num_workers,shuffle=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RMSELoss(nn.Module):\n    def __init__(self, reduction = 'mean', eps = 1e-9):\n        super().__init__()\n        self.mse = nn.MSELoss(reduction = 'none')\n        self.reduction = reduction\n        self.eps = eps\n        \n    def forward(self, y_pred, y_true):\n        loss = torch.sqrt(self.mse(y_pred, y_true) + self.eps)\n        if self.reduction == 'none':\n            loss = loss\n        elif self.reduction == 'sum':\n            loss = loss.sum()\n        elif self.reduction == 'mean':\n            loss = loss.mean()\n        return loss  \n\ndef MCRMSE(y_trues, y_preds):\n    scores = []\n    idxes = y_trues.shape[1]\n    for i in range(idxes):\n        y_true = y_trues[:, i]\n        y_pred = y_preds[:, i]\n        score = mean_squared_error(y_true, y_pred, squared = False)\n        scores.append(score)\n    mcrmse_score = np.mean(scores)\n    return mcrmse_score, scores\n   \n\nclass AverageMeter(object):\n    def __init__(self):\n        self.reset()\n        \n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n        \n    def update(self, val, n = 1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\ndef asMinutes(s):\n    m = math.floor(s / 60)\n    s -= m * 60\n    return f'{int(m)}m {int(s)}s'\n\ndef timeSince(since, percent):\n    now = time.time()\n    s = now - since\n    es = s / (percent)\n    rs = es - s\n    return f'{str(asMinutes(s))} (remain {str(asMinutes(rs))})' \n\ndef get_logger(filename=CFG.OUTPUT_DIR+'train'):\n    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=f\"{filename}.log\")\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\n\nif CFG.loss_func == 'SmoothL1':\n    criterion = nn.SmoothL1Loss(reduction='mean')\nelif CFG.loss_func == 'RMSE':\n    criterion = RMSELoss(reduction='mean')\n    \nlogger= get_logger()\nlogger","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FGM():\n    def __init__(self, model):\n        self.model = model\n        self.backup = {}\n\n    def attack(self, epsilon = 1., emb_name = 'word_embeddings'):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad and emb_name in name:\n                self.backup[name] = param.data.clone()\n                norm = torch.norm(param.grad)\n                if norm != 0:\n                    r_at = epsilon * param.grad / norm\n                    param.data.add_(r_at)\n\n    def restore(self, emb_name = 'word_embeddings'):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad and emb_name in name:\n                assert name in self.backup\n                param.data = self.backup[name]\n            self.backup = {}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sys.path.append('../input/iterativestratification')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nFold = MultilabelStratifiedKFold(n_splits = CFG.n_fold, shuffle = True, random_state = CFG.seed)\nfor n, (train_index, val_index) in enumerate(Fold.split(df, df[CFG.target_cols])):\n    df.loc[val_index, 'fold'] = int(n)\ndf['fold'] = df['fold'].astype(int)\n\nif CFG.debug:\n    CFG.epochs = 2\n    CFG.trn_fold = [0,1]\n    df = df.sample(n = 100, random_state = CFG.seed).reset_index(drop=True)\ndf.head(3)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MeanPooling(nn.Module):\n    def __init__(self):\n        super(MeanPooling, self).__init__()\n        \n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min = 1e-9)\n        mean_embeddings = sum_embeddings/sum_mask\n        return mean_embeddings\n\nclass MaxPooling(nn.Module):\n    def __init__(self):\n        super(MaxPooling, self).__init__()\n        \n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        embeddings = last_hidden_state.clone()\n        embeddings[input_mask_expanded == 0] = -1e4\n        max_embeddings, _ = torch.max(embeddings, dim = 1)\n        return max_embeddings\n    \nclass MinPooling(nn.Module):\n    def __init__(self):\n        super(MinPooling, self).__init__()\n        \n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        embeddings = last_hidden_state.clone()\n        embeddings[input_mask_expanded == 0] = 1e-4\n        min_embeddings, _ = torch.min(embeddings, dim = 1)\n        return min_embeddings\n\n#Attention pooling\nclass AttentionPooling(nn.Module):\n    def __init__(self, in_dim):\n        super().__init__()\n        self.attention = nn.Sequential(\n        nn.Linear(in_dim, in_dim),\n        nn.LayerNorm(in_dim),\n        nn.GELU(),\n        nn.Linear(in_dim, 1),\n        )\n\n    def forward(self, last_hidden_state, attention_mask):\n        w = self.attention(last_hidden_state).float()\n        w[attention_mask==0]=float('-inf')\n        w = torch.softmax(w,1)\n        attention_embeddings = torch.sum(w * last_hidden_state, dim=1)\n        return attention_embeddings\n\n#There may be a bug in my implementation because it does not work well.\nclass WeightedLayerPooling(nn.Module):\n    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n        super(WeightedLayerPooling, self).__init__()\n        self.layer_start = layer_start\n        self.num_hidden_layers = num_hidden_layers\n        self.layer_weights = layer_weights if layer_weights is not None \\\n            else nn.Parameter(\n                torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n            )\n\n    def forward(self, ft_all_layers):\n        all_layer_embedding = torch.stack(ft_all_layers)\n        all_layer_embedding = all_layer_embedding[self.layer_start:, :, :, :]\n\n        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n\n        return weighted_average","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FB3Model(nn.Module):\n    def __init__(self, CFG, config_path = None,pretrained=False):\n        super().__init__()\n        self.CFG = CFG\n        # 设置模型的config文件，根据此配置文件读取预训练模型\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(CFG.model_path, ouput_hidden_states = True)\n            self.config.hidden_dropout = 0.\n            self.config.hidden_dropout_prob = 0.\n            self.config.attention_dropout = 0.\n            self.config.attention_probs_dropout_prob = 0.            \n            \n        else:\n            self.config = torch.load(config_path)   \n        #logger.info(self.config)\n        \n        \n        if pretrained:\n            self.model = AutoModel.from_pretrained(CFG.model_path, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n       \n            \n        if CFG.pooling == 'mean':\n            self.pool = MeanPooling()\n        elif CFG.pooling == 'max':\n            self.pool = MaxPooling()\n        elif CFG.pooling == 'min':\n            self.pool = MinPooling()\n        elif CFG.pooling == 'attention':\n            self.pool = AttentionPooling(self.config.hidden_size)\n        elif CFG.pooling == 'weightedlayer':\n            self.pool = WeightedLayerPooling(self.config.num_hidden_layers, layer_start = CFG.layer_start, layer_weights = None)        \n        # 用一个全连接层得到预测的6类输出\n        self.fc = nn.Linear(self.config.hidden_size, self.CFG.n_targets)\n   \n   # 根据池化方法选择输出\n    def feature(self,inputs):\n        outputs = self.model(**inputs)\n        if CFG.pooling != 'weightedlayer':\n            last_hidden_states = outputs[0]\n            feature = self.pool(last_hidden_states,inputs['attention_mask'])\n        else:\n            all_layer_embeddings = outputs[1]\n            feature = self.pool(all_layer_embeddings)\n            \n        return feature\n    \n    def forward(self,inputs):\n        feature = self.feature(inputs)\n        outout = self.fc(feature)\n        return outout     ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#LLDR\ndef get_optimizer_grouped_parameters(model, layerwise_lr,layerwise_weight_decay,layerwise_lr_decay):\n\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    # initialize lr for task specific layer\n    optimizer_grouped_parameters = [{\"params\": [p for n, p in model.named_parameters() if \"model\" not in n],\n                                     \"weight_decay\": 0.0,\"lr\": layerwise_lr,},]\n    # initialize lrs for every layer\n    layers = [model.model.embeddings] + list(model.model.encoder.layer)\n    layers.reverse()\n    lr = layerwise_lr\n    for layer in layers:\n        optimizer_grouped_parameters += [{\"params\": [p for n, p in layer.named_parameters() if not any(nd in n for nd in no_decay)],\n                                          \"weight_decay\": layerwise_weight_decay,\"lr\": lr,},\n                                         {\"params\": [p for n, p in layer.named_parameters() if any(nd in n for nd in no_decay)],\n                                          \"weight_decay\": 0.0,\"lr\": lr,},]\n        lr *= layerwise_lr_decay\n    return optimizer_grouped_parameters\n                \n    \n# 选择使用线性学习率衰减或者cos学习率衰减\ndef get_scheduler(cfg, optimizer, num_train_steps):\n    if cfg.scheduler == 'linear':\n        scheduler = get_linear_schedule_with_warmup(\n            optimizer, \n            num_warmup_steps = cfg.num_warmup_steps, \n            num_training_steps = num_train_steps\n        )\n    elif cfg.scheduler == 'cosine':\n        scheduler = get_cosine_schedule_with_warmup(\n            optimizer, \n            num_warmup_steps = cfg.num_warmup_steps, \n            num_training_steps = num_train_steps,\n            num_cycles = cfg.num_cycles\n        )\n    return scheduler","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_fn(fold,train_loader, model, criterion, optimizer, epoch, scheduler, device):\n    losses = AverageMeter()\n    model.train()\n    scaler = torch.cuda.amp.GradScaler(enabled = CFG.apex) # 自动混合精度训练\n    start = end = time.time()\n    global_step = 0\n    if CFG.fgm:\n        fgm = FGM(model) # 对抗训练\n\n    for step, (inputs, labels) in enumerate(train_loader):\n        #attention_mask = inputs['attention_mask'].to(device)\n        inputs = collate(inputs)\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        labels = labels.to(device)\n        batch_size = labels.size(0)\n        \n        with torch.cuda.amp.autocast(enabled = CFG.apex):\n            y_preds = model(inputs)\n            loss = criterion(y_preds, labels)\n        if CFG.gradient_accumulation_steps > 1:\n            loss = loss / CFG.gradient_accumulation_steps\n        losses.update(loss.item(), batch_size)\n        scaler.scale(loss).backward()\n        if CFG.unscale:\n            scaler.unscale_(optimizer)\n        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n        \n        #Fast Gradient Method (FGM)\n        if CFG.fgm:\n            fgm.attack()\n            with torch.cuda.amp.autocast(enabled = CFG.apex):\n                y_preds = model(inputs)\n                loss_adv = criterion(y_preds, labels)\n                loss_adv.backward()\n            fgm.restore()\n            \n        \n        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n            global_step += 1\n            scheduler.step()\n        end = time.time()\n        \n        if step % CFG.print_freq == 0 or step == (len(train_loader) - 1):\n            print('Epoch: [{0}][{1}/{2}] '\n                  'Elapsed {remain:s} '\n                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n                  'Grad: {grad_norm:.4f} '\n                  'LR: {lr:.8f} '\n                  .format(epoch + 1, step, len(train_loader), remain = timeSince(start, float(step + 1)/len(train_loader)),\n                          loss = losses,\n                          grad_norm = grad_norm,\n                          lr = scheduler.get_lr()[0]))\n        if CFG.wandb:\n            wandb.log({\" loss\": losses.val,\n                       \" lr\": scheduler.get_lr()[0]})\n    return losses.avg","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def valid_fn(valid_loader, model, criterion, device):\n    losses = AverageMeter()\n    model.eval()\n    preds ,targets= [],[]\n    start = end = time.time()\n    \n    for step, (inputs, labels) in enumerate(valid_loader):\n        inputs = collate(inputs)\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        labels = labels.to(device)\n        batch_size = labels.size(0)\n        \n        with torch.no_grad():\n            y_preds = model(inputs)\n            loss = criterion(y_preds, labels)\n        if CFG.gradient_accumulation_steps > 1:\n            loss = loss / CFG.gradient_accumulation_steps\n        losses.update(loss.item(), batch_size)\n        preds.append(y_preds.to('cpu').numpy())\n        targets.append(labels.to('cpu').numpy())\n        end = time.time()\n        \n        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n            print('EVAL: [{0}/{1}] '\n                  'Elapsed {remain:s} '\n                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n                  .format(step, len(valid_loader),\n                          loss=losses,\n                          remain=timeSince(start, float(step+1)/len(valid_loader))))\n    predictions = np.concatenate(preds)\n    targets=np.concatenate(targets)\n    return losses.avg, predictions","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_loop(df, fold):\n    logger.info(f\"========== fold: {fold} training ==========\")\n    # 加载数据集\n    train_folds = df[df['fold'] != fold].reset_index(drop = True)\n    valid_folds = df[df['fold'] == fold].reset_index(drop = True)\n    valid_labels = valid_folds[CFG.target_cols].values\n    \n    train_encoding,train_label=preprocess(train_folds,tokenizer,True)\n    val_encoding,val_label=preprocess(valid_folds,tokenizer,True)\n    \n    train_dataset = MyDataset(train_encoding,train_label)\n    valid_dataset = MyDataset(val_encoding,val_label)\n    \n    train_loader = DataLoader(train_dataset,batch_size = CFG.batch_size,shuffle = True, \n                              num_workers = CFG.num_workers,pin_memory = True)\n    valid_loader = DataLoader(valid_dataset,batch_size = CFG.batch_size * 2,\n                              shuffle=False,num_workers=CFG.num_workers,pin_memory=True, )\n    \n    model = FB3Model(CFG, config_path = None,pretrained=True) \n    torch.save(model.config, CFG.OUTPUT_DIR +'./config.pth')\n    model.to(CFG.device)  \n    # 加载优化器和调度器\n    from torch.optim import AdamW\n    grouped_optimizer_params = get_optimizer_grouped_parameters(model, \n                               CFG.layerwise_lr,CFG.layerwise_weight_decay,CFG.layerwise_lr_decay)\n    optimizer = AdamW(grouped_optimizer_params,lr = CFG.layerwise_lr,eps = CFG.layerwise_adam_epsilon)\n       \n\n    num_train_steps = len(train_loader) * CFG.epochs\n    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n    best_score = np.inf\n\n    for epoch in range(CFG.epochs): # 开始训练\n\n        start_time = time.time()\n        avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, CFG.device)\n        avg_val_loss, predictions = valid_fn(valid_loader, model, criterion, CFG.device)\n        \n        # scoring\n        score, scores = MCRMSE(valid_labels, predictions)\n        elapsed = time.time() - start_time\n\n        logger.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n        logger.info(f'Epoch {epoch+1} - Score: {score:.4f}  Scores: {scores}')\n        if CFG.wandb:\n            wandb.log({f\"[fold{fold}] epoch\": epoch+1, \n                       f\"[fold{fold}] avg_train_loss\": avg_loss, \n                       f\"[fold{fold}] avg_val_loss\": avg_val_loss,\n                       f\"[fold{fold}] score\": score})\n        \n        if best_score > score:\n            best_score = score\n            logger.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n            torch.save({'model': model.state_dict(),\n                        'predictions': predictions},\n                        CFG.OUTPUT_DIR+f\"_fold{fold}_best.pth\")\n\n    predictions = torch.load(CFG.OUTPUT_DIR+f\"_fold{fold}_best.pth\", \n                             map_location=torch.device('cpu'))['predictions']\n    valid_folds[[f\"pred_{c}\" for c in CFG.target_cols]] = predictions\n\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    return valid_folds # 返回验证集，方便后续看4折的验证结果","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__ == '__main__':\n    \n    def get_result(oof_df):\n        labels = oof_df[CFG.target_cols].values\n        preds = oof_df[[f\"pred_{c}\" for c in CFG.target_cols]].values\n        score, scores = MCRMSE(labels, preds)\n        logger.info(f'Score: {score:<.4f}  Scores: {scores}')\n    \n    oof_df = pd.DataFrame()\n    for fold in range(CFG.n_fold):\n        if fold in CFG.trn_fold:\n            _oof_df = train_loop(df, fold)\n            oof_df = pd.concat([oof_df, _oof_df])\n            get_result(_oof_df)\n    oof_df = oof_df.reset_index(drop=True)\n    logger.info(f\"========== CV ==========\")\n    get_result(oof_df)\n    oof_df.to_pickle(CFG.OUTPUT_DIR+'oof_df.pkl')\n        \n    if CFG.wandb:\n        wandb.finish()","metadata":{},"execution_count":null,"outputs":[]}]}